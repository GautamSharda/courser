[{"title":"XGBoost Part 1 (of 4): Regression","text":"XG boost its extreme and its gradient\nboost stat quest hello I'm Josh stormer\nand welcome to stat quest today we're\ngonna talk about XG boost part 1 we're\ngonna talk about XG boost trees and how\nthey're used for regression note this\nstat quest assumes that you are already\nfamiliar with at least the main ideas of\nhow gradient boost does regression and\nyou should be familiar with at least the\nmain ideas behind regularization if not\ncheck out the quests the links are in\nthe description below\nXG boost is extreme and that means it's\na big machine learning algorithm with\nlots of parts the good news is that each\npart is pretty simple and easy to\nunderstand and we'll go through them one\nstep at a time\nactually I'm assuming that you are\nalready familiar with gradient boost and\nregularization\nso we'll start by learning about XG\nboosts unique regression trees\nbecause this is a big topic will spend\nthree whole stack quests on it in this\nstack quest part 1 will build our","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=0s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"intuition about how XG boost does\nregression with its unique trees\nin part two we'll build our intuition\nabout how XG boost does classification\nand in part three we'll dive into the\nmathematical details and show you how\nour aggression and classification are\nrelated and why creating unique trees\nmakes so much sense\nnote 'xg boost was designed to be used\nwith large complicated datasets however\nto keep the examples from getting out of\nhand\nwe'll use this super simple training\ndata\non the x-axis we have different drug\ndosages\nand on the y-axis we've measured drug\neffectiveness\nthese two observations have relatively\nlarge positive values for drug\neffectiveness and that means that the\ndrug was helpful\nthese two observations have relatively\nlarge negative values for drug\neffectiveness and that means that the\ndrug did more harm than good\nthe very first step in fitting XG boost\nto the training data is to make an\ninitial prediction this prediction can\nbe anything but by default it is 0.5","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=85s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"regardless of whether you're using XG\nboost for regression or classification\nthe prediction 0.5 corresponds to this\nthick black horizontal line and the\nresiduals the differences between the\nobserved and predicted values show us\nhow good the initial prediction is\nnow just like unex treem gradient boost\nXG boosts fits a regression tree to the\nresiduals\nhowever unlike unex treem gradient boost\nwhich typically uses regular\noff-the-shelf regression trees XG boost\nuses a unique regression tree that I\ncall an XG boost tree so let's talk\nabout how to build an XG boost tree for\nregression note there are many ways to\nbuild XG boost trees this video focuses\non the most common way to build them for\nregression\neach tree starts out as a single leaf\nand all of the residuals go to the leaf\nnow we calculate a quality score or\nsimilarity score for the residuals\nsimilarity score equals the sum of the\nresiduals squared over the number of\nresiduals plus lambda note lambda is a","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=168s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"regularization parameter and we'll talk\nmore about that later for now let lambda\nequals zero now we plug the for\nresiduals into the numerator\nand since there are four residuals in\nthe leaf we put a four in the\ndenominator note because we do not\nsquare the residuals before we add them\ntogether in the numerator 7.5 and\nnegative 7.5 cancel each other out in\nother words when we add this residual to\nthis residual they cancel each other out\nlikewise 6.5 cancels out most of\nnegative 10.5 leaving negative 4 squared\nin the numerator\nthus the similarity score for the\nresiduals in the root equals 4 so let's\nput similarity equals 4 up here so we\ncan keep track of it\nnow the question is whether or not we\ncan do a better job clustering similar\nresiduals if we split them into two\ngroups\nto answer this we first focus on the two\nobservations with the lowest dosages\naverage dosage is 15 and that\ncorresponds to this dotted red line\nso we split the observations into two\ngroups based on whether or not the","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=260s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"dosage is less than 15\nthe observation on the far left is the\nonly one with dosage less than 15 so\nit's residual goes to the leaf on the\nleft all of the other residuals go to\nthe leaf on the right\nnow we calculate the similarity score\nfor the leaf on the left by plugging the\n1 residual into the numerator and since\nonly one residual went to the leaf on\nthe Left the number of residuals equals\n1\nlike before we set lambda equal to zero\nand the similarity score for the leaf on\nthe Left equals 110 0.25 so let's put\nsimilarity equals 110 0.25 under the\nleaf so we can keep track of it\nand calculate the similarity score for\nthe residuals that go to the leaf on the\nright\nwe plug in the sum of residuals squared\ninto the numerator and since there are\nthree residuals in the leaf on the right\nwe plug 3 into the denominator\nlike before let's let lambda equal zero\nnote like we saw earlier because we do\nnot square the residuals before we add\nthem together 7.5 and negative 7.5","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=354s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"cancel each other out leaving only one\nresidual 6.5 in the numerator\nthus the similarity score for the\nresiduals in the leaf on the right\nequals 14 point zero eight\nso let's put similarity equals 14 point\nzero eight under the leaf so we can keep\ntrack of it\nnow that we have calculated similarity\nscores for each node we see that when\nthe residuals in a node are very\ndifferent they cancel each other out and\nthe similarity score is relatively small\nin contrast when the residuals are\nsimilar or there is just one of them\nthey do not cancel out and the\nsimilarity score is relatively large\nnow we need to quantify how much better\nthe leaves cluster similar residuals\nthan the root\nwe do this by calculating the gain of\nsplitting the residuals into two groups\ngain is equal to the similarity score\nfor the leaf on the Left plus the\nsimilarity score for the leaf on the\nright\nminus the similarity score for the root\nplugging in the numbers eep-eep eep-eep\neep-eep eep-eep eep-eep gives us 120","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=444s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"point three three small BAM\nnow that we have calculated the gain for\nthe threshold of dosage less than 15 we\ncan compare it to the gain calculated\nfor other thresholds so we shift the\nthreshold over so that it is the average\nof the next two observations and build a\nsimple tree that divides the\nobservations using the new threshold\ndosage less than twenty two point five\nnow we calculate the similarity scores\nfor the leaves\nand calculate the game pppp pppp pppp de\nboop boop the game for dosage less than\ntwenty two point five is four since the\ngain for dosage less than twenty two\npoint five is less than the game for\ndosage less than fifteen dosage less\nthan fifteen is better at splitting the\nresiduals into clusters of similar\nvalues\nnow we shift the threshold over so that\nit is the average of the last two\nobservations and build a simple tree\nthat divides the observations using the\nnew threshold dosage less than 30\nthen we calculate the similarity scores","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=531s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"for the leaves and the gain doot-doot\ndoot-doot doot-doot doot-doot doot-doot\ndoot-doot doot-doot doot-doot\nthe game for dosage less than thirty\nequals fifty six point three three\nagain since the gain for dosage less\nthan 30 is less than the game for dosage\nless than 15 dosage less than 15 is\nbetter at splitting the observations and\nsince we can't shift the threshold over\nany further to the right we are done\ncomparing different thresholds\nand we will use the threshold that gave\nus the largest gain dosage less than 15\nfor the first branch in the tree BAM now\nsince there is only one residual in the\nleaf on the Left we can't split it any\nfurther however we can split the three\nresiduals in the leaf on the right so we\nstart with these two observations and\ntheir average dosage is twenty-two point\nfive which corresponds to this dotted\ngreen line so the first threshold that\nwe try is dosage less than twenty two\npoint five now just like before we\ncalculate the similarity scores for the","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=613s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"leaves note we calculated the similarity\nscore for this node when we figured out\nhow to split the root\nso now we calculate the game dududududu\ndudududududududu and we get gain equals\ntwenty eight point one seven four when\nthe threshold is dosage less than twenty\ntwo point five\nnow we shift the threshold over so that\nit is the average of the last two\nobservations calculate the similarity\nscores for the leaves and the gain\ndoo-doo-doo-doo - and we get gain equals\none hundred forty point one seven which\nis much larger than twenty eight point\none seven when the threshold was dosage\nless than twenty two point five\nso we will use dosage less than 30 as\nthe threshold for this branch note to\nkeep this example from getting out of\nhand\nI've limited the tree depth to two\nlevels and this means we will not split\nthis leaf any further and we are done\nbuilding this tree however the default\nis to allow up to six levels small BAM\nnow we need to talk about how to prune\nthis tree","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=696s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"we prune an XG boost tree based on its\ngain values we start by picking a number\nfor example 130 oh no it's the dreaded\nterminology alert XG Boost calls this\nnumber gamma\nwe then calculate the difference between\nthe gain associated with the lowest\nbranch in the tree and the value for\ngamma\ndifference between the gain and gamma is\nnegative we will remove the branch and\nif the difference between the gain and\ngamma is positive we will not remove the\nbranch in this case when we plug in the\ngame in the value for gamma 130 we get a\npositive number so we will not remove\nthis branch and we are done pruning\nnote the gain for the route 120 point\nthree is less than 130 the value for\ngamma so the difference will be negative\nhowever because we did not remove the\nfirst branch we will not remove the\nroute\nin contrast if we set gamma equal to 150\nthen we would remove this branch because\n140 point 1 7 minus 150 equals a\nnegative number so let's remove this\nbranch\nnow we subtract gamma from the gain for","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=782s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"the route\nsince 120 point 3 3 minus 150 equals a\nnegative number we will remove the root\nand all we would be left with is the\noriginal prediction which is pretty\nextreme pruning\nso while this wasn't the most nuanced\nexample of how an X G boost tree is\npruned I hope you get the idea\nnow let's go back to the original\nresiduals and build a tree just like\nbefore only this time when we calculate\nsimilarity scores we will set lambda\nequal to one\nremember lambda is a regularization\nparameter which means that it is\nintended to reduce the prediction\nsensitivity to individual observations\nnow the similarity score for the root is\n3.2 which is 8/10 of what we got when\nlambda equals 0\nwhen we calculate the similarity score\nfor the leaf on the Left we get fifty\nfive point one two which is half of what\nwe got when lambda equals zero\nand when we calculate the similarity\nscore for the leaf on the right we get\nten point five six which is\nthree-quarters of what we got when\nlambda equals zero","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=880s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"so one thing we see is that when lambda\nis greater than zero the similarity\nscores are smaller and the amount of\ndecrease is inversely proportional to\nthe number of residuals in the node in\nother words the leaf on the Left had\nonly one residual and it had the largest\ndecrease in similarity score 50% in\ncontrast the root had all four residuals\nin the smallest decrease 20% now when we\ncalculate the gain we get 66 which is a\nlot less than 120 point 3 3 the value\nweak out when lambda equals 0\nsimilarly when lambda equals one the\ngame for the next branch is smaller than\nbefore\nnow just for comparison these were the\ngain values when lambda equals zero when\nwe first talked about pruning trees we\nset gamma equal to 130 and because for\nthe lowest branch in the first tree gain\nminus gamma equaled a positive number so\nwe did not prune at all\nnow with lambda equals 1 the values for\ngain are both less than 130 so we would\nprune the whole tree away\nso when lambda is greater than zero it","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=970s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"is easier to prune leaves because the\nvalues for gain are smaller\nnote before we move on I want to\nillustrate one last feature of lambda\nfor this example imagine we split this\nnode into two leaves now let's calculate\nthe similarity scores with lambda equal\nto one for the branch we get sixty five\npoint three for the left leaf we get\ntwenty one point one two\nand for the right leaf we get 28.1 to\nthat means the gain is negative sixteen\npoint zero six\nnow when we decide if we should prune\nthis branch we plug in the game and we\nplug in a value for gamma note if we set\ngamma equal to zero then we will get a\nnegative number and we will prune this\nbranch\neven though gamma equals zero\nin other words setting gamma equal to\nzero does not turn off pruning - on the\nother hand by setting lambda equal to 1\nlambda did what he was supposed to do it\nprevented overfitting the training data\nawesome for now regardless of lambda and\ngamma let's assume that this is the tree","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=1063s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"we are working with and determine the\noutput values for the leaves the output\nvalue equals the sum of the residuals\ndivided by the number of residuals plus\nlambda note the output value equation is\nlike the similarity score except we do\nnot square the sum of the residuals so\nfor this leaf we plug in the residual\nnegative 10.5\nthe number of residuals in the leaf 1\nand the value for the regularization\nparameter lambda\nif lambda equals zero then there is no\nregularization in the output value\nequals negative 10.5\non the other hand if lambda equals one\nthe output value equals negative five\npoint two five\nin other words when lambda is greater\nthan zero then it will reduce the amount\nthat this individual observation adds to\nthe overall prediction\nthus lambda the regularization parameter\nwill reduce the prediction sensitivity\nto this individual observation\nfor now we'll keep things simple and let\nlambda equal zero because this is the\ndefault value and put negative 10.5","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=1163s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"under the leaf so we will remember it\nnow let's calculate the output value for\nthis leaf when lambda equals zero the\noutput value is seven\nin other words when lambda equals zero\nthe output value for a leaf is simply\nthe average of the residuals in that\nleaf\nso we'll put the output value under the\nleaf so we will remember it\nlastly when lambda equals zero the\noutput value for this leaf is negative\nseven point five\nnow at long last the first tree is\ncomplete double BAM since we have built\nour first tree we can make new\npredictions and just like on extreme\ngradient boost XG boost makes new\npredictions by starting with the initial\nprediction and adding the output of the\ntree scaled by a learning rate\noh no it's another dreaded terminology\nalert\nXG boost calls the learning rate Etta\nand the default value is 0.3 so that's\nwhat we'll use thus the new predicted\nvalue for this observation with dosage\nequal to 10 is the original prediction\n0.5 plus the learning rate Etta 0.3","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=1252s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"times the output value negative 10.5 and\nthat gives us negative 2.6 5 so if the\noriginal prediction was 0.5 then this\nwas the original residual\nthe new prediction is negative two point\nsix five and we see that the new\nresidual is smaller than before so we've\ntaken a small step in the right\ndirection\nsimilarly the new prediction for this\nobservation with dosage equal twenty is\nbeep boop boop boop beep boop beep boop\n2.6 and the new residual is smaller than\nbefore so we've taken another small step\nin the right direction\nlikewise the new predictions for the\nremaining observations have smaller\nresiduals than before suggesting each\nsmall step was in the right direction\nBAM\nnow we build another tree based on the\nnew residuals and make new predictions\nthat give us even smaller residuals and\nthen build another tree based on the\nnewest residuals and we keep building\ntrees until the residuals are super\nsmall or we have reached the maximum\nnumber\ntriple bam in summary when building XG","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=1348s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"boost trees for regression we calculate\nsimilarity scores\ngain to determine how to split the data\nprune the tree by calculating the\ndifferences between gain values and a\nuser defined a tree complexity parameter\ngamma\nif the difference is positive then we do\nnot prune if it's negative then we prune\nfor example if we subtract gam off from\nthis game and get a negative value we\nwill prune otherwise were done\nif we prune then we will subtract gamma\nfrom the next game value and work our\nway up the tree then we calculate the\noutput values for the remaining leaves\nand lastly lambda is a regularization\nparameter and when lambda is greater\nthan zero it results in more pruning by\nshrinking the similarity scores and it\nresults in smaller output values for the\nleaves BAM\ntune in next time for XG boost part 2\nwhen we give an overview of how XG boost\ntrees are built for classification it's\ngoing to be totally awesome\nhooray we've made it to the end of\nanother exciting stat quest if you liked","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=1437s"},{"title":"XGBoost Part 1 (of 4): Regression","text":"this stack quest and want to see more\nplease subscribe and if you want to\nsupport stack quest consider\ncontributing to my patreon campaign\nbecoming a channel member buying one or\ntwo of my original songs or a t-shirt or\na hoodie or just donate the links are in\nthe description below alright until next\ntime quest on","link":"https://www.youtube.com/watch?v=OtD8wVaFm6E&t=1524s"}]