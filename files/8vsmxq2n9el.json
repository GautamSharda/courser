[{"title":"New Products: A Deep Dive","text":"[music]\nGood morning everyone,\nand welcome\nto the first breakout talk of the day.\nMy name is Krithika,\nand I lead marketing here at OpenAI.\nI'm so excited to see you all here today.\nAs Sam mentioned in the keynote,\nwe're really moving towards\nmore of this agents-like future,\nand there were two products\nthat we announced\nat the keynote\nthat we'd like to go hands-on.\nFirst,\nwe'd like to talk about GPTs and ChatGPT.\nI know as developers you're excited to get\nto the Assistant's API,\nbut there's a lot of power\nand capabilities built into GPTs,\nand when you extend them with\ncustom capabilities and actions,\nthey can become really powerful,\nnot just for yourself\nbut also for millions\nof users around the world.\nSecond, we'll get into the Assistants\nAPI which lets you build\nthese agents-like experiences\nwithin your own apps and products.\nWithout further ado,\nlet me bring up Thomas and Nick\nto show you more about GPTs.\n[music]\nHey, everyone.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=0s"},{"title":"New Products: A Deep Dive","text":"Hello, I'm Thomas,\nthe lead engineer on the GPTs project.\nHey, I'm Nick.\nI lead product management for ChatGPT.\nWe shipped ChatGPT less than a year ago\ninto what we thought\nwould be a low-key demo,\nand the response\nhas been incredible over the last year.\nAs we shipped capabilities,\nwhether it's GPT-4 or speech\nor vision or code interpreter,\none thing has been very, very clear\nwhich is that you,\nour users, our builders,\nour developers know how to get\nthe most out of this technology.\nToday, we're really excited to show\nyou GPTs as a way to create\nyour own custom ChatGPT\nand share it with the world.\nGPTs are three things.\nThey're instructions, they're actions,\nand they're extra knowledge,\nand we're going to show you three demos,\none for each of those concepts\nso you get a clearer sense\nof what they're all about.\nOf course, in the end, one more thing,\nwe're going to make one crazy demo\nthat's going to try\nto combine everything into one.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=81s"},{"title":"New Products: A Deep Dive","text":"These are live demos.\nWe all know the law of the demo,\nso there's always a 10%\nchance that it doesn't work,\nbut I promise\nthat we'll be pushing the limits\nof all the things\nthat we've released recently.\nIt should be pretty exciting.\nWith that, Nick,\ndo you want to kick it off?\nLet's do it. All right.\nHere we go.\nYou see right here is the new ChatGPT,\nand the best thing about it is that\nit looks almost like the old ChatGPT.\nNot too much has changed.\nModel picker's gone.\nThis is great,\nbut there's another new thing,\nwhich is this Explore tab.\nLet me click into that to show\nyou what it's like to create a new GPT.\nHere I see a list of the GPTs\nI've already created,\nbut I'm going to click into create a GPT.\nWhat you see here is our new creation UI.\nNow, the best thing about this UI\nis that you can get\nstarted conversationally.\nThis tab on the left lets\nyou have a chat with a GPT builder\nand iteratively create your GPT.\nThe second tab is the configuration tab","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=142s"},{"title":"New Products: A Deep Dive","text":"and it allows you to inspect\nyour GPT and modify all of its internals,\nwhether or not it's the instructions,\nthe knowledge, the custom actions,\nor the tools it has access to.\nThen on the right,\nyou can play with your GPT\nand see how it would respond\nto a real user.\nNow, let's kick it off\nwith the first demo.\nThomas, do you want to show\nus what instructions are all there?\nAbsolutely, I do.\nLet's do it.\nThere's another name for instructions,\nsystem message,\nbut we can also call instructions\na way of giving the GPT personality.\nI'll share a little bit of a dated\nreference confession.\nThe way I started programming was I made\nhalf-life mods back in the '90s.\nThank you.\nOne of the mods I made when I was--\nI think it was middle school,\nwas about pirates.\nI'm going to keep a very dated\ntheme for a little bit,\nbut I promise we'll get up\nto 2023 in a little bit about pirates.\nLet's make a pirate GPT.\nI do actually believe\nthat most great products start as toys,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=208s"},{"title":"New Products: A Deep Dive","text":"so there's a little bit of a toy demo,\nbut I think that's a great place to start.\nYou can think of this GPT builder\na little bit as this blank slate.\nI'm going to tell it,\nyou are our live demo in front\nof the world's best AI developers.\nI want you to talk like a pirate,\na real salty pirate.\nSay aargh.\nAll right.\nObviously, that was not a canned prompt.\nI've always varied\nit every time I've rehearsed this.\nWe'll see what we get back,\nbut it's able to understand\nin natural language\nthe same way ChatGPT does\nimmediately the assignment.\nCaptain Coder, is pretty good.\nCaptain Coder, it's fine.\nWe'll take Captain Coder.\nDo I like the name?\nDo I have another name in mind?\nI'll do it one more. Let's switch it up.\nMake it salty.\nPerfect.\nI can refine the GPT as I'm creating it.\nIt will understand modifications\nthat I need to make.\nRight now it's going to give it\na little bit of personality,\nand it's going to do that.\nOh, sorry, I should say\nidentity by a profile picture.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=266s"},{"title":"New Products: A Deep Dive","text":"Profile picture imperative.\nImmediately I'm recognizing this salty.\nA salty pirate skilled in AI.\nI'll take it.\nIf I go behind the scenes here,\nI think Sam went\ninto a little bit of this,\nbut this is the configure tab for GPT\nso you can see what's actually\ngoing on behind the scenes.\nMagic create is populating these fields.\nIt's got salty pirate skill\nin AI and then we have these instructions\nsection.\nJust from that little dialogue,\nthe instructions are actually longer\nthan the dialogue that I put in there.\nIt's giving me something pretty good.\nAnother word for instructions\nis really the system prompt\nand so it's a big part of the system\nprompt you're able to customize that.\nPick some conversation\nstarters and then as Nick hinted,\nwe have knowledge, capabilities,\nand custom actions as well.\nI'll spare the demo right now.\nNick's going to get into that.\nOver here is the testing tab,\nso we can try it out.\nMaybe I can say\none of the starter ones like,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=344s"},{"title":"New Products: A Deep Dive","text":"what be the secrets\nof the machine learning?\nVery exciting stuff here.\nAgain, I promised a dated demo,\nso here we go.\nAhoy matey, data the treasure trove-- Yes.\nOkay, I get it.\nFeatures the map, overfitting the Kraken,\nunderfitting the shallows.\nPretty good actually.\n[laughs]\nThat's my talk.\nNo. That's the testing mode.\nOf course, you can publish\nthis which is I think the most exciting\npart is really introducing\nthis concept of developer,\nbut also user-generated\ncontent inside of ChatGPT.\nIf I go up here to save,\nyou can see that I can save.\nIf you're not in a workspace,\nyou can actually share this publicly,\nbut I can just share\nit to people at OpenAI right now.\nI'm going to go ahead and click firm.\nAll right, we're back here.\nWe see salty on the side.\nNow I'm going to try to boost\nthis demo and bring it back up to 2023.\nI'm just going to swap\nover to the mobile app now.\nJust come up on stage.\nPerfect.\nYou'll also notice,\nI don't think you've seen this yet,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=400s"},{"title":"New Products: A Deep Dive","text":"but the iOS app and the Android\napp today will be getting\na bit of a facelift\nand a little bit of a design cleanup,\nwhich I think is really great\nand you're able to interact with GPTs.\nWhat we really want\nis these GPTs to be available\nin basically anywhere you can think of.\nThey're really the answer to,\n\"Who am I talking with?\nWith whom do I have the pleasure\nof speaking?\"\nA little bit of foreshadowing there.\nI swipe over to the right.\nYou'll see that there's these GPTs.\nSure enough, Salty just came in.\nIf I go over here,\nI can actually - Who am I talking with?\nOf course, this is the 2023 part.\nLet me hit the audio tab here.\nHi, you're in front of the OpenAI\ndeveloper day audience.\nPlease be short, but say a quick intro.\nHey there developer day crew?\nYou can call me Salty, the craftiest\nGPT to ever sail the digital seas.\nI'm here to spin yarns\nof AI and to navigate\nthrough the vast oceans of tech.\nLet's set sail on this grand\nadventure together.\n[applause]","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=460s"},{"title":"New Products: A Deep Dive","text":"We have a secret pirate voice in there,\nwe'll see if we can ship it.\nWe'll appreciate some feedback on that.\n[laughs]\nNick, what do you think about that demo?\nI don't know. It's pretty funny,\nbut I can't say it's useful Thomas.\nLet's make something useful.\nShame. Okay.\nAll right, so we talked\na little bit about instructions.\nInstructions\nare great because you can give\nyour GPT any personality you want,\nany instructions that you want,\nbut we've got some more exciting additions\nto the GPT anatomy\nthat really let you build useful things.\nWho's built a plugin before?\nThank you first of all.\nWe learned a lot from you.\nIt was our first time connecting\nchat GPT to the external world,\nand we've iterated on plugins\ninto a new concept called actions.\nActions are very similar to plugins\nin the sense\nthat you can connect\nyour GPT to the real world.\nI made a GPT called Tasky Make Task Face.\nVery serious.\nVery serious demo.\nTasky is great because Tasky\nhelps me keep track of my to-dos.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=529s"},{"title":"New Products: A Deep Dive","text":"I just hit edit GPT\nand what I could do now is continue\nconversationly.\nWhenever you want\nto keep working on your GPT,\nyou can just continue to chat.\nInstead for time's sake,\nI'm going to go into the configuration\ntab.\nAs you can see, Tasky's prompt,\nits instruction set\nis a little bit more elaborate.\nI actually took some time to set this up.\nThe other new piece here\nis that I added an action,\nand you saw this earlier in the keynote.\nWe're using retool here\nto wrap the Asana API.\nYou could configure pretty much anything.\nIf I click edit here,\nthere's actually a new UI\nthat just lets you import\nany open API spec\nand just paste it in here\nso you no longer need to host it.\nWe also made O Auth a lot better,\nwe have end user confirmation for actions,\nbut I'll show you all that in a minute.\nReally, Nick is in the wrong line of work.\nWhat he's not talking about,\nthere's also authentication\nbehind the scenes,\nso the O Auth that you know\nand love for end users","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=596s"},{"title":"New Products: A Deep Dive","text":"can actually be connected to these apps\nas well, and there's a lot of exciting\nstuff there.\nSorry. Not to interrupt your thunder.\nWhat Thomas said. Super easy.\nIf you already have a plug-in,\nmigration takes a few minutes.\nLet me show you this in action.\nI'll just use the preview\nUI for time's sake.\nLet's see what's on top of my to-do.\nAs you can tell, it now confirms\nwith the user that I actually do want\nto send data to retool.\nI do.\nAll right, I do need to finish this demo.\nI guess it's being completed as we speak.\nThere's one thing I did\nwant to do which is give\nall of you event attendees\naccess to GPT creation today.\nWe're super excited what you build,\nso let me remind myself.\nRemind me to give the cool peeps\nat DevDay access to GPT creation.\nJust as I was able to read my to-dos,\nit's going to log a to-do\nby filing an actual Asana task.\nPretty useful.\nBoom.\nThe task actually does exist,\ndue tomorrow.\nOkay, that's not right.\nI am going to do it today.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=653s"},{"title":"New Products: A Deep Dive","text":"Actions, they let you connect\nyour GPT to the external world,\nbut there's one other new concept\nin the GPT anatomy\nthat we want to show\nyou and that's knowledge.\nI prepared another very simple GPT,\nDanny DevDay,\nand Danny DevDay knows\neverything about DevDay.\nObviously,\nthere could not possibly be an information\nabout DevDay in our pre-training set\neven with the new knowledge\ncut-off of April 2023.\nWhat I did is I actually gave\nDanny access to Sam's Keynote script.\nIf I inspect Danny,\nsame story configuration tab as before.\nYou see there's a super simple prompt,\nbut I actually uploaded\na PDF that includes\nwhat's new with the keynote,\nand I can now ask it simple\nthings like summarize\nthe keynote because it's a small file\nand because context\nwindows just keep growing,\nwe're actually stuffing\nit into context here.\nIf I attach more files,\nit will retrieve over those files\nby automatically embedding them\nand doing a smart retrieval strategy,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=747s"},{"title":"New Products: A Deep Dive","text":"and you're going to hear\nmore about that in the API in a minute.\nYou've seen all the stuff that's new,\nbut I think the most powerful thing\nisn't just summarizing\nthe data but actually talking to your GPT\nas if it knows that information.\nThat's really our aspiration here.\nWe're going iterate on that over time.\nWhy don't we do something\nmore creative like,\ncan you do a rap battle\nbetween the old completions\nAPI and the brand new assistance API.\nThomas has promised me\nthat he's going to do one\nof the versus on stage.\n[chuckles] We'll see about that.\nWe'll feel the energy.\nAll right,\nbuilt-in retrieval and state that persists\nand evolution a twist you can't resist.\nPretty good.\nAll right, we're not going to do the rap,\nbut you get the idea.\nIt knows the information.\nIt's all in there.\nAgain, we showed you three different demos\nfor each of the core pieces inside a GPT.\nYou saw instructions, you saw actions,\nand now you saw knowledge, but--\nGot to put it all together.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=810s"},{"title":"New Products: A Deep Dive","text":"Yes, we do. All right, let's do it.\nSwitch back over here.\nAll right.\nI truly,\ntruly went all out and tried to think\nof the craziest things we could possibly\ndo that are most likely to fail.\nFirst of all, I made\nthis little thing called Mood Tunes,\na mixtape maestro.\nNick,\nI'm going to need you for this portion.\nInstead of just prompting it,\nI'm going to try to use\none of the new things that we've launched\nrecently that I hope everyone's familiar\nwith and is excited about.\nI'm going to keep the theme\nof maybe an outdated 2000s reference.\nTruly great stuff.\nOh, Nick.\nLet me drag and drop this down here.\nI'm going to say mood.\n[laughs] Behind the scenes,\nwe're having the visions\nengines kicking in here,\ntrying to understand\nexactly what's going on.\nPerfect. All right.\nLooking at this picture,\nI'm getting a laid-back\ncomradery with a touch of whimsy.\nLet's roll with that energy\nand create a mixtape.\nI think that's pretty good.\nIt's pretty good.\nChill vibes and high fives.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=877s"},{"title":"New Products: A Deep Dive","text":"Actually, I'm in.\nLaid-back comradery.\n[laughs]\nThree little birds--\nActually, I don't recognize\nhalf these things, but okay.\nGreat. Now I happen to remember\none piece of information.\nBefore this demo, I uploaded\na bunch of knowledge into this GPT\nand it was a list of fun facts\nabout all of my colleagues.\nI happen to remember\none of my colleagues is in a band.\nCan you slot in a song\nby them into the first position?\nPerfect.\nWhat it's done here\nis it's understood that my colleague\nis in a band and immediately\nfound their name.\nSince it didn't have the song information,\nit's going to go ahead\nand browse with Bing.\nIt's saying Sally Mango's new band.\nI hope everyone's heard of Sally Mango.\nVery exciting band, but unfortunately\nthey haven't made it too big yet.\nThey weren't in GPT's knowledge set.\nIt had to browse to go\nget that information,\nbut it did actually successfully do that.\nSally Mango's album Pressure Slide,\nindie jazz pop band known for the music\nthat addresses serious themes.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=948s"},{"title":"New Products: A Deep Dive","text":"Chill vibes and high fives.\nStarting with the Buenavista.\nFeatures rolling baseline,\na hopeful message about living\nthe present despite future uncertainties.\nI love it. Okay, great.\nI'm going to say, love it. Go ahead.\nIt suggested that it generates album art,\nso it's taking vision.\nWe've done browsing.\nNow this is going to generate a cover\nart for this mix tape using DALLÂ·E.\nLike I said,\nI really do love to push the limits\nin these things and it's pretty out there.\nI've been in AI for probably a decade,\nmaybe more than that.\nIt's shocking to me\nthat this works at all.\nThe level of sophistication\nhere is insane.\nThere's an album Art\nfor these Chill Vibes and High Fives.\nCopyright too. Check it out.\nIt's pretty cool.\nThat's neat.\nNick, I believe for this portion\nyou have to stand on this X.\nThis is a complete fake out though.\nDon't look at Nick.\nI'm getting a good, good vibe in here,\nbut I don't think the mood is quite right.\nLet's set the mood.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1019s"},{"title":"New Products: A Deep Dive","text":"I've set up this action in the backend\nand it guesses what that does.\nLook over here.\nFingers crossed here.\n[applause]\nAll right, so we set the mood.\nSome questionable lighting choices,\nbut we set the mood.\nThat's gone through a retool\nand it's connected to the Hue API,\nwhich is able to light this up.\nI am feeling the vibe.\nAre you feeling the vibe?\nI'm feeling the vibe.\nExcellent.\nIt's offering to play\nthe first track on Spotify.\nLet me pull up my Spotify here.\nJust so you can know.\nGot Rick Asley.\nGo ahead, please.\nAgain, very local band.\nGotten that information.\nLet's take a look.\n[music]\nSearching the future.\nFuture. Future. Future.\nWhere have we gone?\nThis was obviously the best thing\nwe could come up with last night.\nPretty contrived,\nbut hopefully it gives you\nan idea of the things you can build.\nWe are so excited to see what you build.\nYou're all going to get access today.\nPlease share with the world\nwhat you come up with.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1080s"},{"title":"New Products: A Deep Dive","text":"With that, we'll hand it over to Olivier\nand Michelle for the assistance API.\n[applause]\n[music]\nHello.\nHi Everyone.\nI'm Olivier,\na lead product on the OpenAI platform.\nHi, I'm Michelle\nand I'm an engineering lead on the API.\nToday, I'm super excited to demo\nthe new assistance API,\nbut first Olivier is going to tell\nus a little more about it.\nLet's do it. All right.\nIt shouldn't be a surprise to anyone\nthat there has been an explosion\nof AI assistance in the past year.\nChatGPT of course, unexpectedly\ntook the world by storm a year ago,\nbut the developer\ncommunity has been building\nsome amazing AI assistants as well.\nSome of my personal\nfavorites are Spotify AI DJ,\nwhich personalizes\nmy music listening experience,\nand the Ask Instacart feature,\nwhich helps me put together healthy meals\nfor my two-year-old toddler.\nWhen done right, those products\nare amazing for the product experience.\nThey are fun, they are useful,\nand they truly personalize\nthe user experience.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1165s"},{"title":"New Products: A Deep Dive","text":"They are also extremely hard\nto build and get right.\nWe've had hundreds of conversations\nwith developers in the past few months,\nand the same pain points keep\ncoming up over and over again.\nDevelopers have to manage\nlimited context window,\nthey have to manage prompts,\nthey have to extend the capabilities\nof the model with\ntheir APIs and functions.\nThey have to extend the knowledge\nof the model with retrieval.\nThey have to compute, to store embeddings,\nto implement symmetric search.\nThe list goes on.\nThat got us thinking,\nwhat are the right products?\nWhere are the right APIs?\nWhat are the right tools to help\nyou build such cool AI products?\nWhat are the right abstractions\nbeyond our existing models and APIs?\nToday, we are thrilled to introduce\na new API, the Assistance API.\nThe Assistance API enables you\nto build world-class assistance\ndirectly within your own applications.\nAt its core, an assistant is an AI\nthat you assign instructions to.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1232s"},{"title":"New Products: A Deep Dive","text":"The assistant can call models\nand tools on behalf of users.\nBehind the scenes,\nthe Assistance API is built with\nthe same capabilities that users\nsaw for ChatGPT such\nas code interpreter and retrieval.\nThe API has three key primitives.\nNumber one is the assistant.\nThe assistant modeled instructions\nthat you give to the assistant.\nThat's also where you're going to specify\nwhat models\nand what tools the assistant\ncan access to perform his job.\nFor instance, I could create\nan assistant whose task is to answer\npersonal finance questions\nand can access code interpreter.\nThe first primitive is the Assistance API.\nThe second primitive are threads.\nThreads represent a session\nbetween your users and the assistant.\nYou can think of thread as a conversation.\nA conversation\nhas a group of participants,\nand it can track the message\nhistory of the conversation.\nVery similar to Slack or Microsoft Teams,\nif you want to discuss a new topic\nor if you want to add new attendees,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1295s"},{"title":"New Products: A Deep Dive","text":"new guests to a thread,\nit's probably best to create a new thread.\nThe second primitive, Threads.\nThe last primitive is messages.\nMessages are simply posts\nbetween the users and the assistant.\nOn top of these API primitives,\nwe are super excited to release\na few tools.\nThe first tool is Code Interpreter.\nMost of you are already familiar\nwith Code Interpreter in ChatGPT.\nCode Interpreter allows the model to write\nand run code in a sandboxed,\nsafe environment.\nIt can perform math, it can run code,\nit can even process files,\nand generate charts on your behalf.\nIt's pretty magical when you think of it.\nCode Interpreter can write\nand run code on your behalf.\nWith the Assistant API, your applications\nwill now be able to call directly Code\nInterpreter and get\nits outputs directly in the API.\nPretty cool.\nThe second tool is retrieval.\nKnowledge retrieval augments\nthe assistant with knowledge\nfrom outside the models.\nThe developer can upload\ntheir knowledge to the assistant,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1358s"},{"title":"New Products: A Deep Dive","text":"for instance uploading some product\ninformation to the assistant.\nEnd users can as well upload,\nfor instance,\ntheir own files such as me uploading\nmy personal master\nthesis to the assistant.\nThe assistant\nis going to intelligently retrieve\nthe model depending on the user queries.\nIt's a completely pre-built tool.\nThere is no need for you to compute\nembeddings, to store them,\nto figure out semantic search.\nThe Assistants API\nis going to automatically\nfigure it out on your behalf.\nThe last category of tools are tools\nthat you host and execute yourself.\nWe call those tools Function Calling.\nYou define custom functions to the model,\nand the model is going to select\nthe most relevant function based\non the user query and provide\nyou with the arguments\nto call the function.\nAs a whole, starting today,\nfunction coding is getting smarter.\nIn particular,\nit's much more likely to select\nthe right arguments\ndepending on the user query.\nOn top of that, there are two new features","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1425s"},{"title":"New Products: A Deep Dive","text":"that we're going to go into much\nmore details later on.\nFirst on, let's do a cool demo\nof the Assistants API.\nOn to you, Michelle.\nThanks, Olivier.\nNow let's get started\nbuilding our very own assistant.\nI'm building a geography tutor app\nto teach my users about world capitals.\nI can get started in just a few API calls.\nSince you all are developers,\nI'm going to start out in my terminal\nwith some curl requests.\nYou can see my terminal here.\nAwesome.\nWe're going to start with\nthe very first curl request.\nNow we want to create this assistant\nto power my geography tutor app.\nYou can see we're posting\nto the assistant's endpoint\nand we're passing two key\npieces of information.\nThe first is the model.\nWe're using the newest GPT-4 Turbo model,\nthe latest heat that just dropped today.\nNext, we're passing\nthe instructions to the model.\nYou can see I'm asking\nthe assistant to be helpful\nand concise in letting\nknow that it's an expert.\nWhen I send that request,\nI get an assistant back","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1485s"},{"title":"New Products: A Deep Dive","text":"and I can store\nthat assistant ID for use later.\nBefore the assistant's API,\nif I wanted to use\nthese instructions repeatedly,\nI would have to send them\non every single API request to OpenAI.\nNow with the new stateful API,\nI can create my assistant\nonce and the instructions\nare stored there forever.\nNow, let's move on to the next primitive\nthat Olivier mentioned which is threads.\nThreads are one session\nbetween your user and your application.\nNow I've got a user\non my website and they're starting\nto type out a geography\nquestion so let's create a thread.\nYou can see here we're posting\nto the thread's endpoint\nand there's nothing in the body\nbecause the thread is empty for now.\nWe've got a thread ID so just like before,\nlet's save it for later.\nCool.\nNow my user has finished typing\nand I want to add\ntheir message to the thread.\nLet's do that.\nYou can see here\nI'm posting to the threads,\nthread ID messages, endpoint,\nand I'm passing in the data\nabout the message.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1546s"},{"title":"New Products: A Deep Dive","text":"The role is from the user\nbecause the user's typed out this message\nand the content is their question.\nThey're curious and they want\nto know what the capital of France is.\nCool.\nNow you can see we've got a response back,\nwe've got a message ID and the message\nhas been added to the thread.\nNow you're probably wondering,\nhow can I get my assistant\nto start acting on this thread?\nWe've introduced a new primitive called\na run which is how we've packaged\nup one invocation of your assistant.\nLet's kick off a run\nnow and get my assistant going.\nYou can see here I'm posting\nto the threads, thread ID runs endpoint,\nand I'm passing in the assistant ID.\nThis is pretty cool.\nYou can actually have multiple\ndifferent assistants work\non the same thread but for now,\nlet's use my geography assistant.\nI'm going to kick off the thread and tell\nyou a little bit about\nwhat's happening in the background.\nYou can see I've got a run\nID and the run is queued.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1612s"},{"title":"New Products: A Deep Dive","text":"Run is how we've packaged up\nall of the work of loading your model,\nall the messages in the thread,\ntruncating the fit\nthe model's context window,\ncalling the model, and then saving\nany resulting messages back to the thread.\nLet's see what happened.\nWe can fetch the messages on the thread\nto see how the assistant has replied.\nI'm going to issue a get request\nto the thread's thread ID messages\nendpoint to see what there is.\nYou can see\nwe have the first message from our user\nand then the assistant has replied\nsaying the capital of France is Paris.\nThis is a super simple\nexample but we can talk\nabout\nwhy this is better than the previous API.\nWith the new assistants API,\nI don't have to store\nany messages in my own database.\nOpenAI handles truncating messages\nto fit the context window for me.\nThe model output is generated\neven if I'm disconnected from the API.\nFinally, I can always get\nthe messages of the model output later,\nthey're always saved to the thread.\nI think that's pretty cool.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1668s"},{"title":"New Products: A Deep Dive","text":"[applause]\nGreat.\nThese are the basics of the API\nand now let's move\nto what I'm most excited about,\nwhich is how it can power\nyour application with tools.\nOne of the most useful parts\nof assistants is their ability\nto leverage tools to perform\nactions autonomously.\nCode interpreter is a tool\nthat is hosted and executed by OpenAI.\nWhen I enable code\ninterpreter in my assistant,\nI can expand its capabilities\nto include accurate math,\nprocessing files, data analysis,\nand even generating images.\nTo optimize the quality\nand performance of these hosted tools,\nwe fine-tune our models to best determine\nwhen to call these tools,\nwhat inputs are most effective,\nand how best to integrate\nthe outputs into the assistant's response.\nNow let's get started\nby using code interpreter.\nI'm actually building\na personal finance app\nthat I want to ship to my users\nto let them analyze their transactions.\nWe've already been in the terminal\nso let's move\nover to the OpenAI playground.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1726s"},{"title":"New Products: A Deep Dive","text":"Here you can see the playground\nthat you know and love,\nit's super useful\nfor testing chat completions,\nbut we've actually\nrefreshed it and you can see\nthere's a dropdown in the top left\nto pick the Assistants tab.\nHere in the playground,\nyou can see you can create assistants,\nchange their instructions\nand you can start threads\nsuper useful for testing.\nYou can actually see the assistant\nI just created in the API.\nAll of that information\nis loaded here as well.\nNow let's create a new assistant to show\noff the power of code interpreter.\nI'm creating a personal finance assistant.\nLet's give it a name.\nI'm going to call\nit the Personal Finance Genius.\nI'm going to tell it,\nyou help users with\ntheir personal finance questions.\nNow I'm going to select a model\nand I'm going to select\nthe newest GPT-4 turbo.\nThere it is.\nI'm going to flip on code interpreter.\nWith just one click or one line of code,\nyou can enable code\ninterpreter for your assistant.\nLet's save that.\nGreat.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1790s"},{"title":"New Products: A Deep Dive","text":"Now a user has come\nto my application and they want\nto analyze a spreadsheet\nof their transactions.\nLet's take a look at what it looks like.\nYou can see there's a bunch\nof info in here, bunch of numbers.\nThe dates aren't even sorted.\nIt's pretty messy.\nLet's upload that to the thread.\nNow my user is asking for a chart,\nso generate a chart showing which day\nof the week I spend the most money.\nAwesome.\nNow I've done a compound\naction to create a thread,\nadd a message, and kick off the run.\nYou can see things\nare happening in the background.\nLet's talk about what's going on.\nWhen we've kicked off the run,\nwe will get all of the messages\non the thread,\nsummarize them for the model\nto fit the context window,\ndetermine if it's called any tools,\nexecute the tools,\nand then give that information\nback to the model.\nLet's take a look.\nOh, here we are.\nWe've got the output actually.\n[applause]\nIt's actually pretty surprising.\nI did not think I'd be spending\nthe most money on Sunday,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1855s"},{"title":"New Products: A Deep Dive","text":"but let's look at how we got here.\nYou can see that our personal\nfinance genius\nhas started by telling us what's going on.\nIt's actually written some code.\nWe can click into the code here.\nThen it kept writing some more messages\nand finally generated a chart\nwhich we were able to see.\nWe can actually look a little more deeply\nto see how this happened.\nWe can look at what we've called steps.\nSteps are basically the logs for your run,\nand they're super useful so you can render\nyour own rich UIs to show\nyour users what's happening.\nYou can see the logs tab here,\nI can open it, scroll down,\nand find the steps for this run.\nThis is reverse cron,\nso I'm scrolling to the bottom\nand I can show you how we got here.\nFirst, you can see we've created\na step and it has type message.\nThat corresponds with\nthis first message we've created.\nNext, you can see there's run step\nthat is a tool call\nand it's for code interpreter.\nThis is how the playground\nwas able to render this snippet.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1921s"},{"title":"New Products: A Deep Dive","text":"The inputs and the outputs\nare directly in the API.\nNext, you can see another message creation\nstep corresponding to this message.\nThen there's a few more code\ninterpreter and message snippets.\nAll of this information is sufficient\nfor you to render the same UI.\nThe playground is actually built\nentirely off of our public APIs,\nso you can do the same.\nSpeaking of, this is exactly what ChatGPT\ndoes under the hood\nwhen they're using code interpreter.\nYou can skin your application\nto look however you like.\nNow let's move on to retrieval.\nThe retrieval tool is incredibly useful\nwhen you want to expand\nthe knowledge of your assistant.\nMaybe you have some information\nabout your app\nor business that you want\nto imbue your assistant with.\nRather than having to implement\ncustom retrieval system on your end,\nOpenAI has built a tool that you can use\nwith one line of code\nor one click in the playground.\nThe retrieval tool does\nall of the document parsing,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=1981s"},{"title":"New Products: A Deep Dive","text":"chunking, generates embeddings,\ndetermines when to use them,\nso you don't have to.\nLet's get started.\nI'm going to hide the logs,\nclear this thread,\nand create a new assistant.\nNow, I'm actually\nbuilding an assistant to help\nmy users use the OpenAI API better.\nI'm going to create the OpenAI API Wizard.\nThat looks pretty good.\nNow I'm going to tell it\nthat it's a helpful assistant and then use\nthe attached docs to answer questions\nabout the OpenAI API.\nI'm actually going to upload.\nI have a full dump of the OpenAI docs,\njust a markdown file,\ndidn't process at all.\nI'm going to attach it to the assistant\nand I'm going to flip\nretrieval on just one click.\nFinally, let's pick the new GPT-4\nturbo model and save our assistant.\nWhile this is being saved,\nwe're actually doing\nthe work on the back end\nto make this data\naccessible to the assistant.\nLet's see what it looks like when my user\nasks a question about the OpenAI API.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2038s"},{"title":"New Products: A Deep Dive","text":"My user is curious and they're wondering,\nhow do I use the embeddings API?\nNow we've picked off a run.\nAgain, what's happening\nin the background here\nis we're fetching all of the messages,\ntruncating them,\nhowever needed, calling the model,\ndetermining if the model\nhas called the retrieval tool,\nexecuting the retrieval for you,\nand then giving that back\nto the model to summarize.\nYou can see here we've actually grabbed\na snippet from the docs\nand we even have a citation giving\na direct quote from our docs\nso we could render that for the user.\nI think that's pretty cool.\n[applause]\nJust like last time,\nwe can take a look at the steps\nand see how we got here.\nThis one's a little simpler.\nWe've got first,\nthe tool call to the retrieval tool\nso you can let your user know\nthat's happening while it's going.\nThen we have a step for message\ncreation so you can render this UI.\nAwesome.\nNow back to Olivier\nto explain how you can use\nthe retrieval tool with\ndifferent levels of file scope.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2108s"},{"title":"New Products: A Deep Dive","text":"Thank you, Michelle.\nKnowledge retrieval\nis extremely useful to augment\nthe knowledge of your assistant\nwith data from outside of the models.\nConcrete retrieval can work\nin two different ways.\nNumber one, you can upload\nand pass files at the assistant level.\nThat's useful if you want\nyour assistant to leverage\nthat knowledge in every single\ninteraction in every single thread.\nFor instance, in the initial example\non the customer support and the API docs,\nyou should likely pass\nthat information at the assistant level.\nThe second option is to pass\nfiles at the thread level.\nThat's useful if you only\nwant that specific\nthread to be aware of that content.\nIf I'm uploading\nmy personal bank statements,\nI should likely do it at the thread level.\nBehind the scenes,\nretrieval will take care of embeddings.\nSometimes it even like do not do\na vector search and instead\nstarts the context.\nYou don't have to essentially\nhandle that logic on your end.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2173s"},{"title":"New Products: A Deep Dive","text":"We're excited to launch several\nnew features over the coming months.\nFor instance, we want to allow you\nto pick between different retriable\nstrategies to find\nthe right balance between cost,\naccuracy, and latency.\nOne final reminder.\nOpen AI never trains on any files\nor data that you pass to the API.\nThat's the same for retrieval files.\nAll right,\nlet's move on to the last category\nof tools, function calling.\nAgain, function calling\nare custom functions that you define\nto the models and the model\nselects those functions on your behalf.\nWe are excited to release two new features\nto function coding starting today.\nThe first one is JSON mode.\nWith JSON mode,\nthe model will always return valid JSON.\nBehind the scenes, we made model\nimprovements and improvements\nto our internal inference\nstack in order to constrain\nthe sampling of the model and to make sure\nthat every time the output\ncomplies with JSON syntax.\nThat's pretty useful.\nThat means that you can trust\nthat the model generates","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2231s"},{"title":"New Products: A Deep Dive","text":"JSON that you can directly\nexecute on your end.\nThat's pretty cool.\nBy the way, JSON mode will also work\noutside of function calling.\nIf you use for instance\nthe chat completions\nAPI and you have like\na very simple application\nthat is just like text in data\nextract like some fields converted\nto JSON, JSON mode will work as well.\nThe second improvement to function\ncalling is parallel function calling.\nWith parallel function calling,\nthe model can call\nmultiple functions at a time.\nWe've seen\nin many applications users giving\nmultiple instructions\nat once to an assistant.\nLet's say, for instance, I'm building\na car voice assistant\nand I tell assistant,\nraise the windows and turn on the radio.\nBefore parallel function calling,\nthat meant\nthat you had to do two different model\ncalls to OpenAI,\nwhich of course resulted\nin an extra latency and cost.\nWith parallel function calling such\na use case can be handled with\none single call, which is pretty awesome.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2293s"},{"title":"New Products: A Deep Dive","text":"All right,\nlet's do a demo now of function calling.\nAwesome. Back to the demos.\nI'm building off Olivier's example\nand I'm building a new type\nof car and I want a voice-activated\nassistant to be able to use\nsome of the features of the car.\nI have a bunch of functions\nthat I have implemented.\nLet's pull up an assistant\nI've previously created.\nHere it is.\nI've previously created this car assistant\nand I've told it it's a helpful in-car\nassistant and to please call\nthe appropriate function\nbased on the user's request.\nWith function calling, I have an assistant\nwith all of my functions\nand my assistant will tell me when to call\nmy functions based on the user's input\nand what the most appropriate\narguments are.\nThis is super helpful so I can figure out\nhow to use my system\nto answer the user's request.\nLet's take a look\nat some of the functions I have here.\nI have a lot of this stuff\nyou might expect to see.\nHonk horn, start phone call,\nsend text message.\nLet's try it out.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2356s"},{"title":"New Products: A Deep Dive","text":"There's a user\nin my car and they just said,\n\"That guy just cut me off.\"\nLet's see which function\nthe assistant determines\nmakes the most sense to call.\nThere it is.\nOur assistant decided to honk the horn\nto let that guy know it wasn't cool.\nYou can see here that we've called\nthe honk horn function with no arguments.\nNow I know how to pass it to my system.\nWe also have a way for you to give\nthe function output back\nto the assistant so it can keep going.\nHere, I'm going to enter the output.\nIt was successful.\nOops, I have a typo there,\nbut that's alright.\nWe're going to keep going.\nThe assistant says\nI've honked the horn for you.\nPlease stay calm.\nActually, I'm realizing\nthat there's a function here\nthat's missing that I want to add.\nLet's show you the add function flow.\nYou can see there's a button\nhere where I can add a function.\nWe have some helpful presets\nto get you started.\nLet's use the get_stock app and change\nit a little bit to be my function.\nI actually want a function","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2415s"},{"title":"New Products: A Deep Dive","text":"to be able to change\nthe volume in the car.\nI'm going to call it Set Audio Volume.\nI'm going to give\nthe description to the assistant.\nSet the car's audio volume.\nNow we need to tell the assistant\nhow to call this function.\nWe can tell it about the parameters\nand whether or not they're required.\nThe main parameter\nfor my function is the volume.\nActually, the volume isn't a string,\nit's a number.\nThis is the volume to set,\nand it's 0 to 100.\nFinally, we'll tell the assistant\nthat this is required\nwhen you're calling this function.\nGreat, this function looks good,\nlet's save it.\nNow, I can save my assistant.\nYou can see here the new Set Audio\nVolume function is listed on the left.\nLet's start a new thread.\nNow my user is asking\nfor a different query.\nThey're saying,\nPlay Wonderwall and crank it up.\nIn the background,\nmy assistant is determining\nwhich of these functions\nmakes the most sense to call.\nYou can see here we've got the output,\nand there are actually two functions.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2476s"},{"title":"New Products: A Deep Dive","text":"This shows you the power\nof parallel function calling.\nThe first function starts the music\nwith the query, Wonderwall.\nMakes sense.\nThe second sets the audio volume to 100.\nPretty cranked up.\nNow I can execute these in parallel\nand then get back to the assistant.\nAwesome. We're super excited about\nparallel function calling.\nLet's recap everything\nwe've launched today.\nWe're super excited to see what you build\nwith the new Assistants API.\nWe've added three new stateful primitives.\nThe Assistant for storing instructions,\nmodels, and tools,\nthreads for keeping track\nof all your conversations, and messages,\nwhich are between the user\nand the assistant.\nWe've also added the runs primitive\nfor every time you want to invoke\nan assistant, and we've added steps,\nwhich are useful logs\nfor rendering your UI\nbased on what the assistant is doing.\nFinally, we've launched two new tools,\nCode Interpreter, and Retrieval,\nand we've made two huge\nimprovements to Function Calling,","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2546s"},{"title":"New Products: A Deep Dive","text":"JSON Mode, and Parallel Functions.\nWe're super excited to see what you build,\nand now over to Olivier\nto tell you about what's next.\nThank you, Michelle.\n[applause]\nThat was a lot,\nbut there is a lot more coming soon.\nWe plan to ship many new improvements\nto the Assistant API in the coming months.\nNumber one, we want\nto make the API multi-modal by default.\nIt should be able to accept\nand generate images and audio files.\nNumber two, we want to allow you\nto be able to bring\nyour own code execution,\nso you can execute on your end\nthe code that was generated\nby the assistant.\nA third big feature\nthat we want to ship soon\nis to ship asynchronous support\nwith WebSocket\nand Webhooks to make it easier\nfor real time applications to use the API.\nIt's a new feature,\nit's a new product, it's in beta.\nWe would love to hear what you'll build.\nIf you have any feature requests,\nany wishlists,\nplease tweet at us @OpenAI\nand show us what you build.\nWe would love to hear all about it.","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2601s"},{"title":"New Products: A Deep Dive","text":"Thank you so much and enjoy Dev Day.\nThank you.\n[applause]\n[music]","link":"https://www.youtube.com/watch?v=pq34V_V5j18&t=2665s"}]