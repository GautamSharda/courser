[{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"There is an enigmatic and\npowerful mathematical operation\nat work inside everything from\ncomputer graphics\nand neural networks, to quantum physics.\nIt's simple enough for high school students to grasp\nyet so complex that even seasoned \nmathematicians haven't mastered it.\nThis operation is called matrix multiplication.\nMatrix multiplication is a very\nfundamental operation in\nmathematics that appears in many\ncomputations in engineering\nand in physics.\nA matrix is a two dimensional\narray of numbers on which you\ncan perform operations like\naddition and multiplication.\nResearchers have long sought\nmore efficient ways\nto multiply matrices together.\nSo if you even just make that a\nlittle bit faster, larger\nproblems come into reach.\nWhere for now, we would say that's too big to\nbe computable in reasonable time.\nHowever, actually finding faster\nmatrix multiplication methods is\na huge challenge.\nBut thanks to a new tool, \nresearchers have finally broken","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=1s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"a long standing matrix multiplication record,\none that's more than 50 years old.\nWhat's their secret weapon?\nStudents of linear algebra are\ntaught a method for multiplying\nmatrices based on a centuries\nold algorithm.\nIt goes like this.\nMultiply elements from the\nfirst row of matrix A\nand the first column of matrix B\nand add them to get the first element of matrix C.\nThen repeat for the first row of matrix A\nand the second column of matrix B, and\nadd them for the second element in matrix C.\nAnd so on.\nMultiplying two two\nby two matrices this way,\ntakes eight multiplication steps.\nMultiplying any two N by N matrices with the standard\nalgorithm takes N-cubed steps.\nWhich is why applying this\nmethod to pairs of larger\nmatrices quickly becomes\nunwieldy.\nIf you take a matrix that is\ntwice as big, then you have to\nhave a computation time that is\neight times more.\nSo you can imagine, if you doubled it a\ncouple of times, then you take\neight times more a couple of\ntimes, and you will very soon","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=59s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"reach the limits of what a\ncomputer can do.\nEnter Volker Strassen,\na German mathematician known for his\nwork analyzing algorithms.\nIn 1969, he discovered a new algorithm to multiply two by two matrices\nthat requires only seven multiplication steps.\nGoing from eight down to seven multiplications\nmay seem trivial,\nand the new addition steps look more complicated.\nBut Strassen's algorithm offers dramatic\ncomputational savings for larger matrices.\nThat's because when multiplying large\nmatrices, they can be broken down into smaller ones.\nFor example, an eight by eight\nmatrix can reduce to a series of\nnested two by two matrices.\nSo Strassen's savings applied to these smaller matrix\nmultiplications propagate over and over.\nApplying Strassen's to an eight by eight matrix results\nin a third fewer multiplication\nsteps compared to the standard  algorithm.\nFor very large matrices, these savings vastly\noutweigh the computation costs of the extra additions.","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=123s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"A year after Strassen invented his algorithm, \nIBM researcher Shmuel Winograd\nproved it was impossible to use six or fewer multiplications\nto multiply two by two matrices,\nthus also proving that Strassens, with its\nseven multiplications is the best solution.\nFor half a century the most\nefficient method known for\nmultiplying two matrices of any\nreasonable size was to break\nthem down and apply Strassen's algorithm.\nThat was until October 2022, a new algorithm\nwas revealed that beat\nStrassen's, specifically for multiplying two four by four  matrices\nwhere the elements are\nonly zero or one.\nThis new algorithm made it possible to\nmultiply large matrices even faster\nby breaking them into four by four matrices \ninstead of two by two's.\nSo who or what was behind This\nbreakthrough?\nThis new algorithm was\ndiscovered by Google's\nartificial intelligence research\nlab, DeepMind.\nFor more than a decade, DeepMind has garnered\nattention for training AI systems","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=184s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"to master a host of games, everything from Atari Pong to chess.\nThen, in 2016, DeepMind's AlphaGo achieved\nwhat was considered impossible at the time,\nit defeated the top ranked human Go player, Lee Sedol , in\na best of five match.\nThis victory shattered the limited notion of what's possible for computers to achieve.\nDeepMind then set its sights on a problem even more challenging than Go.\nI was like very surprised that\neven for very small cases, we\ndon't even know what's the optimal \nway of doing matrix multiplication.\nAnd at some point, we realized\nthat this is actually a very good fit for machine learning techniques\nTo tackle matrix multiplication.\nDeepMind started with an\nalgorithm descended from AlphaGo\ncalled AlphaTensor.\nAlphaTensor is built on a\nreinforcement learning algorithm\ncalled AlphaZero.\nSo what one needs to do is to go really beyond the AlphaZero\nreinforcement learning algorithm\nand to tackle this huge search\nspace and to develop techniques","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=245s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"to find this, these needles in a\nvery, very large haystack.\nAlphaTensor isn't the first\ncomputer program to assist with\nmathematical research.\nIn 1976, two mathematicians proved what's called the Four Color Theorem using a computer.\nThe theorem states, you only need four\ncolors to fill in any map so no\nneighboring regions match.\nThe pair verified their proof by processing all 1936 required\ncases\nrequiring more than 1000  hours of computing time.\nBack then the larger mathematical community\nwas not prepared to cede logical reasoning to a machine.\nHowever, the field has since come a long way.\nAlphaTensor was trained with a technique called\nreinforcement learning, which is kind of like\nplaying a game.\nReinforcement Learning strategically penalizes\nand rewards an AI system as it\nexperiments with different ways\nto achieve its given task,\ndriving the program towards an\noptimal solution.\nBut what kind of game should AlphaTensor play\nin search of more efficient\nmatrix multiplication algorithms?","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=314s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"This is where the term tensor in\nAlphaTensor comes into play.\nA tensor is just an array of\nnumbers with any number of  dimensions.\nVectors are 1D tensors, and matrices are 2D tensors.\nThe process of multiplying any two matrices of a given size\ncan be described by a single unique 3D tensor.\nFor example, when multiplying any two, two by two matrices,\nwe can build the corresponding 3D tensor.\nEach dimension of this cube represents one of the matrices,\neach element in the cube can be one zero or negative one.\nThe matrix product C is created by combining elements\nfrom matrices A and B, like this.\nAnd so on.\nUntil you have the full matrix\nmultiplication tensor.\nNow, you can use a process\ncalled tensor decomposition to\nbreak down this 3D tensor into\nbuilding blocks.\nSimilar to taking apart a cube puzzle.\nOne natural way to break tensors down is into what's called\nrank-1 tensors,\nwhich are just products of vectors.\nThe trick is each rank-1 tensor here describes a multiplication step","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=378s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"in a matrix multiplication algorithm.\nFor example, this rank-1 tensor represents the\nfirst multiplication step in the\nstandard algorithm: A1 times B1.\nThe next rank-1 tensor\nrepresents A2 times B3.\nAdding these two rank one tensors\nyields the first element in the product C1.\nHere are the next two rank-1\ntensors, representing the\nmultiplications, A1 times B2 and\nA2 times B4, which form C2.\nEventually, the entire standard\nalgorithm with its eight\nmultiplication steps is\nrepresented by decomposing the\n3D tensor into eight rank-1 tensors.\nThese all add back up into the original 3D tensor.\nBut it's possible to decompose a\n3D tensor in different ways.\nStrassen's seven multiplication\nsteps for the same two by two\nmatrix multiplication looks like this.\nThese rank-1 tensors are more complex.\nAnd this is still a full decomposition, but in fewer steps,\nwhich add backup to the original tensor.\nSo the fewer rank-1 tensors, you use to\nfully decompose a 3D tensor,","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=457s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"the fewer multiplication steps used\nin the tensor's corresponding matrix multiplication.\nDeepMind's construction of a\nsingle-player game\nfor  AlphaTensor to play, and learn\nfrom was key.\nFind an algorithm for this matrix multiplication\nthat requires the fewest\nmultiplication steps possible...\nis a vague request.\nBut it becomes a clearly defined\ncomputer task\nonce it's formulated as: decompose this 3D tensor\nusing as few unique rank-1 tensors as possible,\nIt's really hard to describe what, what the search space looks like.\nIn the particular\ncase of matrix multiplication,\nthat's, that's quite, it's quite convenient to formulate it in that space,\nbecause then we can\ndeploy our search techniques\nand our machine learning techniques\nin order to search in that very,\nvery large, but formalizable\nsearch spaces.\nAlphaTensor's play was simple.\nIt was programmed to guess rank-1 tensors to subtract from\nthe original 3D tensor,\nto  decompose it down to zero.\nThe fewer rank one tensors it used,","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=522s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"the more rewards that got.\nEach rank-1 tensor that you\nremove from from the 3D tensor,\nis has a cost, has a cost of let's say one.\nAnd so we want to figure out what's the way of\nachieving the goal with the fewest penalties.\nAnd so that's what the system is trying to learn how to do\nit's learning to estimate, well when I'm in this kind of configuration,\nroughly how many penalties do I think I'm going to incur\nbefore I get to the goal?\nBut tensor decomposition is not\nan easy game to master.\nFor even a three by three matrix multiplication\nwith only elements zero or one,\nthe number of possible tensor decompositions\nexceeds the number of atoms in the universe.\nStill, over the course of its\ntraining,\nAlphaTensor started to home in on patterns to decompose\nthe tensor efficiently.\nWithin minutes it rediscovered\nStrassen's algorithm.\nThen the program went even further.\nIt beat Strassen's algorithm for\nmultiplying two, four by four matrices\nin modulo-2, where the\nelements are only zero or one,","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=584s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"breaking the 50 year record.\nInstead of the standard algorithms 64 multiplication steps\nor Strassen's 49,\nAlphaTensor's algorithm used only 47 multiplications.\nAlphaTensor also discovered thousands of other new fast algorithms,\nincluding ones for five by five matrices in modulo-2 .\nSo, will programs like AlphaTensor,\nchurning away in server rooms\npulling out new mathematical discoveries from lines of code\nmake mathematicians obsolete?\nUltimately, I think this will not replace like the\nmathematicians or, or anything like that.\nThis I think provides a good tool that can help\nmathematicians find new results\nand guide their intuition.\nThat's exactly what happened just days after the AlphaTensor results\nwere first published in the journal Nature.\nTwo mathematicians in Austria, Manuel, Kauers and Jakob Moosbauer\nused AlphaTensors 96-step, five by five matrix multiplication algorithm\nas inspiration to push even further.\nAnd then Jakob suggested we just take the algorithm","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=647s"},{"title":"How AI Discovered a Faster Matrix Multiplication Algorithm","text":"that the AlphaTensor people found\nas a starting point and see whether\nthere's something in the\nneighborhood of this, and we\ncould have an additional drop.\nAnd indeed, that that was the case.\nSo it was a very short computation for,\nfor our new mechanism that was able to reduce the 96 to 95.\nAnd this we then published in the arxiv paper.\nThe right way to look at this\nis, as a fantastic example of a\ncollaboration between a particular kind of technology\nand mathematicians.\nThe true potential for human and\nartificial intelligence collaboration\nis a frontier that is only now being fully  explored.\nI don't think people can be made\nirrelevant by any of this kind of work\nI think it empowers people to do more.","link":"https://www.youtube.com/watch?v=fDAPJ7rvcUw&t=718s"}]